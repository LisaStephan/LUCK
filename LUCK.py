import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import NearestNeighbors
from copy import deepcopy
from sklearn.metrics.pairwise import paired_euclidean_distances
import traceback


def get_normed_vector(vector):
    """
    Computes the normed vector.

    :param vector: Vector.
    :type vector: ndarray of shape (n_features,)

    :return: Normed vector.
    :rtype: ndarray of shape (n_features,)
    """

    vector = np.array(vector)
    length = np.sqrt(sum(vector * vector))
    if length == 0 or len(vector) == 1:
        return vector
    return (1 / length) * vector


def get_average_vector(vectors, return_variation=False):
    """
    Computes the average vector of a set of vectors and the variation.

    :param vectors: Set of vectors.
    :type vectors: ndarray of shape (n_samples, n_features)

    :param return_variation: Returns variation if True.
    :type return_variation: bool, default=False

    :return: Average vector and if return_variation is True then also variation.
    :rtype: ndarray of shape (n_features,) if return_variation is False, else list [ndarray of shape (n_features), int]
    """

    vectors = [get_normed_vector(v) for v in vectors]
    average_vector = np.zeros(np.size(vectors[0]))
    for vector in vectors:
        average_vector = average_vector + vector
    n = 1 / vectors.__len__()
    average_vector = [n * (c) for c in average_vector]
    average_vector_normed = get_normed_vector(average_vector)
    result = [average_vector_normed]

    variation = sum((1 - abs(np.dot(vectors, average_vector_normed))) ** 2)
    variation = variation / (vectors.__len__())
    result.append(variation)

    if not return_variation:
        return result[0]
    return result


class LUCK:
    """
    LUCK clustering.

    :param threshold: Threshold for the variation in dynamic kNN.
    :type threshold: float

    :param algo: Cluster algorithm. Selected cluster algorithm must have a metric parameter to specify the metric used to calculate the distance between instances.
    :type algo: object

    :ivar dynamic_kNN_result: {"normalized_input_data": data, "label": label, "variation_values_list": variation_values_list, "min_k": min_k}
        data: ndarray of shape (n_samples, n_features)
            Normalized dataset.
        label: ndarray of shape (n_sampels,)
            Labels of each point generated by the dynamic knN. -2 if it is an outlier, otherwise -3.
        variation_values_list: ndarray of shape (n_samples of the whole data - min_k, n_samples)
            Variation values with increasing k until reaching the threshold.
        min_k: int
            Start value for k at dynamic kNN.
    :vartype dynamic_kNN_result: dict

    :ivar data_expanded: Dataset extended with the components of average vectors.
    :vartype data_expanded: ndarray of shape (n_samples, 2*n_features)

    :ivar fitted_estimator: Fitted estimator of the selected cluster algorithm.
    :vartype fitted_estimator: object

    :ivar labels: Labels of each point.
    :vartype labels: ndarry of shape (n_samples,)
    """

    def __init__(self, threshold, algo):
        if threshold > 1 or threshold < 0:
            raise ValueError("threashold must be between 0 and 1")
        else:
            self.threshold = threshold
        self.algo = algo
        self.algo.set_params(metric=self.scalar_length_metric)

    def fit(self, data):
        """
        Computes a LUCK clustering.

        Extends the data set by one vector per data point, which reflects the average over the connection vectors to the k nearest neighbors.
        With the extended data set now a Clustering is performed.

        :param data: Training instances to cluster.
        :type data: ndarray of shape (n_samples, n_features)

        :return: Fitted estimator.
        :rtype: self
        """

        try:

            data = np.array(data)

            # determine a order for the vector standardization
            order = []
            dims = list(range(data.shape[1]))
            dims_with_data = {}
            for dim in dims:
                dims_with_data.update({dim: data[:, dim]})

            def fill_order(dimensions_with_data):
                """
                Determines the order of dimensions for vector standardization.

                :param dimensions_with_data: {dimension_number, ndarray of shape (n_samples,)}
                    Dictionary with the data for every dimension.
                :type dimensions_with_data: dict

                :return:
                """

                while len(dimensions_with_data) > 0:
                    max_values = []
                    finish = False
                    for dimension in dimensions_with_data.keys():
                        if len(dimensions_with_data[dimension]) == 0:
                            order.append(dimension)
                            finish = True
                        else:
                            max_values.append(np.max(dimensions_with_data[dimension]))
                    if finish:
                        break
                    max_values = np.array(max_values)
                    max_dimensions = []
                    for dimension in dimensions_with_data:
                        if np.max(dimensions_with_data[dimension]) == np.max(max_values):
                            max_dimensions.append(dimension)
                    if len(max_dimensions) == 1:
                        order.append(max_dimensions[0])
                        dimensions_with_data.pop(max_dimensions[0])
                    else:
                        dimensions_with_data_sup = {}
                        for dimension in max_dimensions:
                            data = dimensions_with_data[dimension].tolist()
                            data.remove(np.max(max_values))
                            dimensions_with_data_sup.update({dimension: np.array(data)})
                            dimensions_with_data.pop(dimension)
                        fill_order(dimensions_with_data_sup)

            fill_order(dims_with_data)
            self._order = order

            # normalization
            data_normalize = data.astype(float)
            data_normalize = MinMaxScaler().fit_transform(data_normalize)

            # dynamic kNN
            data_expanded, indices, all_labels = self.dynamic_kNN(data_normalize)

            if len(data_expanded) == 0:
                self.labels = all_labels

            data_algo = data_expanded[np.any(data_expanded[:, int(data_expanded.shape[1] / 2):], axis=1)]

            self.fitted_estimator = self.algo.fit(data_algo)
            labels = self.fitted_estimator.labels_

            all_labels[all_labels == -3] = labels

            self.data_expanded = data_expanded
            self.labels = all_labels

        except Exception as e:
            print("An error has occurred while fitting the luck estimator!")
            traceback.print_exc()
        return self

    def get_connection_vector(self, a, b):
        """
        Calculates the vector between two points standardized with respect to the direction.

        :param a: Point.
        :type a: nparray of shape (n_features,)

        :param b: Point.
        :type b: nparray of shape (n_features,)

        :return: Vector between point a and point b.
        :rtype: nparray of shape (n_features,)
        """

        # compute vector
        vector = b - a

        # standardize vector
        for coordinate in self._order:
            if vector[coordinate] < 0:
                vector = np.array([-1 * c for c in vector])
                break
            elif vector[coordinate] > 0:
                break

        return vector

    ####################################################################################################################
    ## K Nearest Neighbor

    def _variable_kNN_part(self, data, min_k, indices, data_range):
        """
        Calculates the average vector over the connection vectors to the k nearest neighbors for a part of the points, where k is the smallest value for each point where the variation over the connection vectors falls below a certain threshold.

        :param data: Whole dataset.
        :type data: ndarray of shape (n_sampels, n_features)

        :param min_k: Start value for k.
        :type min_k: int

        :param indices: Indices of all nearest neighbors for each point ordered by the ascending order by the distanace between point and neighbor.
        :type indices: ndarray of shape (n_samples, n_samples)

        :param data_range:
            number: int
                Number of the considered data part.
            range: range
                Range of the considered points.
        :type data_range: tuple of (number, range)

        :return: number, variation_values, data_part_with_vector, indices, labels
            number: int
                Number of the considered data part.
            variation_values: nparray of shape (n_samples of the whole data - min_k, n_samples of the data part)
                Variation values with increasing k until reaching the threshold for the considered data.
            data_part_with_vector: nparray of shape (n_samples of the data part, 2*n_features)
                Datasetpart extended with the components of average vectors.
            indices: nparray of shape (n_samples of the data part, k)
                Indices of the k nearest neighbors for a part of the point. k can be different for each point.
            labels: list
                Labels of each point generated by this method. -2 if it is an outlier, otherwise -3.
        """

        variation_values_list_part = np.full(((np.shape(indices)[1] - min_k), len(data_range[1])), None)
        data_with_vector_part = []
        neighbor_indices_part = []
        label = []

        for index in data_range[1]:
            point = data[index]
            connection_vectors = []
            k = min_k
            var = self.threshold + 1
            while var > self.threshold and k < np.shape(indices)[1]:
                for arrow_point_index in range(connection_vectors.__len__() + 1, k + 1):
                    arrow_point = data[indices[index, arrow_point_index]]
                    vector = self.get_connection_vector(point, arrow_point)
                    connection_vectors.append(vector)
                average_vector, var = get_average_vector(connection_vectors, return_variation=True)
                variation_values_list_part[k - min_k, index - data_range[1][0]] = var
                k = k + 1
            if not (k == np.shape(indices)[1] and var > self.threshold):
                neighbor_indices_part.append(indices[index, :k])
                data_with_vector_part.append(np.concatenate((point, average_vector)))
                label.append(-3)
            else:
                neighbor_indices_part.append(indices[index, :1])
                data_with_vector_part.append(np.concatenate((point, np.zeros(point.shape))))
                label.append(-2)

        return data_range[0], variation_values_list_part, np.array(data_with_vector_part), np.array(
            neighbor_indices_part), label

    def dynamic_kNN(self, data):
        """
        Calculates the average vector over the connection vectors to the k nearest neighbors for each point, where k is the smallest value for each point where the variation over the connection vectors falls below a certain threshold.

        :param data: Dataset.
        :type data: ndarray of shape (n_samples, n_features)


        :return: data_with_vector, neighbor_indices, labels
            data_with_vector: ndarray of shape (n_samples, 2*n_features)
                Dataset extended with the components of average vectors.
            neighbor_indices: ndarray of shape (n_samples,)
                Indices of the k nearest neighbors for each datapoint. k can be different for each point.
            labels: ndarray of shape (n_sampels,)
                Labels of each point generated by this method. -2 if it is an outlier, otherwise -3.
        """

        if self.threshold >= 1:
            min_k = np.shape(data)[0] - 1
        else:
            min_k = int(np.shape(data)[0] * self.threshold)
            if min_k < 2:
                min_k = 2

        nbrs = NearestNeighbors(n_neighbors=np.shape(data)[0], algorithm='ball_tree').fit(data)
        distances, indices = nbrs.kneighbors(data)
        new_indices = []
        for i in range(len(indices)):
            row = indices[i]
            new_indices.append(np.insert(row[row != i], 0, i))
        indices = np.array(new_indices)

        from concurrent import futures
        from functools import partial

        parts = 16
        n_parts = int(np.shape(data)[0] / parts)
        ranges = []
        for p in range(parts - 1):
            ranges.append((p, range(p * n_parts, (p + 1) * n_parts)))
        ranges.append((parts - 1, range((parts - 1) * n_parts, np.shape(data)[0])))

        with futures.ProcessPoolExecutor(max_workers=4) as executors:
            wait_for = [executors.submit(partial(self._variable_kNN_part, data, min_k, indices), r) for r in
                        ranges]
            results = [np.array(f.result()) for f in futures.as_completed(wait_for)]

        results = np.array(results)
        results = results[results[:, 0].argsort()]

        variation_values_list = np.column_stack([row for row in results[:, 1] if len(row) != 0])
        data_with_vector = np.row_stack([row for row in results[:, 2] if len(row) != 0])
        neighbor_indices = []
        for row in results[:, 3]:
            if len(row) != 0:
                neighbor_indices.extend(np.array(row))
        neighbor_indices = np.array(neighbor_indices)
        label = np.array([y for x in results[:, 4] for y in x])

        self.dynamic_kNN_result = {"normalized_input_data": data, "label": label, "variation_values_list": variation_values_list, "min_k": min_k}

        return np.array(data_with_vector), neighbor_indices, label

    ####################################################################################################################
    ## Metric

    def scalar_length_metric(self, x, y):
        """
        Calculates the distance between two points with their vectors based on the scalars between the direction vectors and between the connection vector of the two points and the direction vector of a point, taking account of the lenght of the connection vector.

        ||v_x o v_y| - (|v_x o v_xy| + |v_y o v_xy|)/2| * dist(x,y)^2

        :param x: Point, of the form [x_1, x_2, ..., x_n, vx_1, vx_2, ..., vx_n], where x_i are coordinates and vx_i are the coordinates of the normed direction vector.
        :type x: ndarray of shape (n_features,)

        :param y: Point, of the form [y_1, y_2, ..., y_n, vy_1, vy_2, ..., vy_n], where y_i are coordinates and vy_i are the coordinates of the normed direction vector.
        :type y: ndarray  of shape (n_features,)

        :return: Distance between the two points.
        :rtype: float

        """

        if (x == y).all():
            return 0
        v_length = int(np.size(x) / 2)
        vx = x[v_length:]
        vy = y[v_length:]
        xy = self.get_connection_vector(x[:v_length], y[:v_length])
        xy_amount = np.linalg.norm(xy)
        xy = get_normed_vector(xy)
        distance = 0
        try:
            distance = abs(abs(np.dot(vx, vy)) - (abs(np.dot(vx, xy)) + abs(np.dot(vy, xy))) / 2)
        except Exception as e:
            print(e)
            print("x: ", x)
            print("y: ", y)
            print("xy: ", xy)
        distance = distance * xy_amount * xy_amount
        return distance

########################################################################################################################
## Kmeans


class KMEANS:
    """
    K-Means clustering.

    :param k: The number of clusters to form as well as the number of centroids to generate.
    :type k: int

    :param max_iter: Maximum number of iterations of the K-Means algorithm for a single run.
    :type max_iter: int, default=300

    :param metrix: The metric to use when calculating distances between instances in a feature array.
    :type metric: function, default=sklearn.metrics.paired_euclidean_distances

    :ivar cluster_centers_: Coordinates of cluster centers.
    :vartype cluster_centers_: ndarray of shape (n_clusters, n_features)

    :ivar labels_: Labels of each point.
    :vartype labels_: ndarry of shape (n_samples,)
    """

    def __init__(self, k, max_iter=300, metric=paired_euclidean_distances):
        self.k = k
        self.max_iter = max_iter
        self.metric = metric

    def fit(self, data):
        """
        Computes a K-Means clustering.

        :param data: Training instances to cluster.
        :type data: ndarray of shape (n_samples, n_features)

        :return: Fitted estimator.
        :rtype: self
        """

        # Number of training data
        n = data.shape[0]
        # Number of features in the data
        c = data.shape[1]

        # Generate random centers, here we use sigma and mean to ensure it represent the whole data
        avg_v, var_v = get_average_vector(data[:, int(c / 2):], return_variation=True)
        mean = []
        mean.extend(np.mean(data[:, :int(c / 2)], axis=0))
        mean.extend(avg_v)
        std = []
        std.extend(np.std(data[:, :int(c / 2)], axis=0))
        std.extend([var_v] * int(c / 2))
        np.random.seed(1)
        centers = np.random.randn(self.k, c) * std + mean
        centers[:, int(c / 2):] = [get_normed_vector(row) for row in centers[:, int(c / 2):]]

        centers_old = np.zeros(centers.shape)  # to store old centers
        centers_new = deepcopy(centers)  # Store new centers

        clusters = np.zeros(n)
        distances = np.zeros((n, self.k))

        differences = []
        for i in range(self.k):
            differences.append(self.metric(centers_new[i], centers_old[i]))
        error = np.linalg.norm(differences)

        iter_count = 0
        # When, after an update, the estimate of that center stays the same, exit loop
        while error != 0 and iter_count <= self.max_iter:

            # Measure the distance to every center
            for i in range(self.k):
                differences = []
                for point in data:
                    differences.append(self.metric(point, centers_new[i]))
                distances[:, i] = differences
            # Assign all training data to closest center
            clusters = np.argmin(distances, axis=1)

            centers_old = deepcopy(centers_new)
            # Calculate mean for every cluster and update the center
            for i in range(self.k):
                if len(data[clusters == i]) != 0:
                    new_center = []
                    avg_v = get_average_vector(data[clusters == i, int(c / 2):])
                    new_center.extend(np.mean(data[clusters == i, :int(c / 2)], axis=0))
                    new_center.extend(avg_v)
                    centers_new[i] = new_center

            differences = []
            for i in range(self.k):
                differences.append(self.metric(centers_new[i], centers_old[i]))
            error = np.linalg.norm(differences)

            iter_count += 1

        self.labels_ = clusters
        self.cluster_centers_ = centers_new
        return self

    def set_params(self, **params):
        """
        Set the parameters of this estimator.

        :param params: Estimator parameters.
        :type params: dict

        :return: Estimator instance.
        :rtype: self
        """

        if not params:
            return self
        for key, value in params.items():
            if(key in self.__dict__.keys()):
                self.__dict__[key] = value
            else:
                raise ValueError('Invalid parameter %s for estimator %s. ' % (key, self))

        return self
